#+STARTUP: latexpreview
#+STARTUP: inlineimages


* The equation to solve:
\begin{equation}
\begin{aligned}
\begin{cases}
\label{eq:NS-n1}
\left[\frac{\partial{v}}{\partial{t}} + v\frac{\partial{v}}{\partial{x}} \right] = \frac{1}{\rho{}}\dfrac{\partial \left(\mu \frac{\partial{v}}{\partial{x}} \right)}{\partial{x}} - \left(\frac{c_0^2}{\rho{}}\right)\dfrac{\partial{\rho}}{\partial{x}} + \frac{V(\rho) - v}{\tau} \\\\
     \dfrac{\partial{\rho}}{\partial{t}} + \dfrac{\partial{\left( \rho{}v \right)}}{\partial{x}}=0
\end{cases}
   \end{aligned}
 \end{equation}

\begin{equation}
\begin{aligned}
q(x,t)=\rho(x,t)v(x,t)
\end{aligned}
\end{equation}

* Boundary Conditions:
\begin{equation}
\begin{aligned}
q(0,t) &= q(L,t)\\
v(0,t) &= v(L,t),\quad \dfrac{\partial{v}}{\partial{x}}\biggr\rvert_0 = \dfrac{\partial{v}}{\partial{x}}\biggr\rvert_L  
\end{aligned}
\end{equation}

From the definition of $q$ and the boundary condition (I),
\begin{equation}
\begin{aligned}
&\rho(0,t)v(0,t) = \rho(L,t)v(L,t) \\
&\implies \rho(0,t) = \rho(L,t)
\end{aligned}
\end{equation}

* Examples from NeuralPDE.jl
** System of PDEs

#+begin_src julia :session main :result output
  using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux, Plots
  using Quadrature,Cubature
  import ModelingToolkit: Interval, infimum, supremum

  @parameters t, x
  @variables u1(..), u2(..), u3(..)
  Dt = Differential(t)
  Dtt = Differential(t)^2
  Dx = Differential(x)
  Dxx = Differential(x)^2

  eqs = [Dtt(u1(t,x)) ~ Dxx(u1(t,x)) + u3(t,x)*sin(pi*x),
         Dtt(u2(t,x)) ~ Dxx(u2(t,x)) + u3(t,x)*cos(pi*x),
         0. ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x) - exp(-t)]

  bcs = [u1(0,x) ~ sin(pi*x),
         u2(0,x) ~ cos(pi*x),
         Dt(u1(0,x)) ~ -sin(pi*x),
         Dt(u2(0,x)) ~ -cos(pi*x),
         u1(t,0) ~ 0.,
         u2(t,0) ~ exp(-t),
         u1(t,1) ~ 0.,
         u2(t,1) ~ -exp(-t)]


  # Space and time domains
  domains = [t ∈ Interval(0.0,1.0),
             x ∈ Interval(0.0,1.0)]

  # Neural network
  input_ = length(domains)
  n = 15
  chain =[FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1)) for _ in 1:3]
  initθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))

  _strategy = QuadratureTraining()
  discretization = PhysicsInformedNN(chain, _strategy, init_params= initθ)

  pde_system = PDESystem(eqs,bcs,domains,[t,x],[u1,u2,u3])
  prob = discretize(pde_system,discretization)
  sym_prob = symbolic_discretize(pde_system,discretization)

  pde_inner_loss_functions = prob.f.f.loss_function.pde_loss_function.pde_loss_functions.contents
  bcs_inner_loss_functions = prob.f.f.loss_function.bcs_loss_function.bc_loss_functions.contents

  cb = function (p,l)
      println("loss: ", l )
      println("pde_losses: ", map(l_ -> l_(p), pde_inner_loss_functions))
      println("bcs_losses: ", map(l_ -> l_(p), bcs_inner_loss_functions))
      return false
  end

  res = GalacticOptim.solve(prob,BFGS(); cb = cb, maxiters=5000)

  phi = discretization.phi
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output
xs,ys = [infimum(domain):dx/10:domain.domain.upper for domain in domains]
analytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)

u_predict = reshape([first(phi([x,y],res.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))
u_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))
diff_u = abs.(u_predict .- u_real)

using Plots
p1 = plot(xs, ys, u_real, linetype=:contourf,title = "analytic");
p2 = plot(xs, ys, u_predict, linetype=:contourf,title = "predict");
p3 = plot(xs, ys, diff_u,linetype=:contourf,title = "error");
plot(p1,p2,p3)
#+end_src

#+RESULTS:

* Kerner - Car Flow

# Let $V(\rho)=1.5*(1-p/\hat{p})^2$, so $p \to \hat{\rho} \implies V(\hat{\rho}) \to 0$.
# Let, $V(\rho) = v_h * (e^{-\frac{(\rho - \rho_h)}{\tau}}) + \delta{v}$.

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  using NeuralPDE, Flux, ModelingToolkit, GalacticOptim, Optim, DiffEqFlux
  import ModelingToolkit: Interval, infimum, supremum
  import Flux: flatten, params
#+end_src

#+RESULTS:

# V(ρ)=1.5*(1-ρ/2)²;

 # $V(\rho) = v_h * (e^{-\frac{(\rho - \rho_h)}{\tau}}) + \delta{v}$

We are trying such parameters that,
$[-1 -\frac{\rho_h}{c_0}\xi(\rho_h)]\rho_h>(\frac{2\pi{}l}{L})^2$ in which $\xi(\rho_h)= \dfrac{dV(\rho_h)}{dt}$

# \begin{equation}
# \begin{aligned}
# \dfrac{dV(\rho_h)}{dt}= \dfrac{v_h (\rho - \rho_h)}{\tau^2}(e^{-\frac{(\rho - \rho_h)}{\tau}})
# \end{aligned}
# \end{equation}

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  @parameters t, x, N, L, ρ_hat, μ, c₀, τ, L, l,vₕ, k, m, ω, λ, γ
  @variables v(..), ρ(..)
  # ρ_hat=0.89;
  m=1;
  μ=1; #choose as we like
  τ=1; #choose as we like 
  # l=sqrt(μ*τ/ρ_hat);

  N = 168; 
  ρₕ = 0.168;
  L=N/ρₕ; 
  δρ₀ = 0.02;
  δv₀ = 0.01;
  vₕ = 5.0461*((1+exp((ρₕ-0.25)/0.06))^-1 - 3.72*10^-6);

  # vhat(ρ)= 5.0461*((1+exp((ρ-0.25)/0.06))^-1 - 3.72*10^-6);
  # using Roots
  # find_zero(vhat, (-5,5))
  # 1.0001069901803379

  # ρₕ=N/L;
  k=2π/L;

  c₀= 1.8634; 
  Dt = Differential(t)
  Dx = Differential(x)
  Dxx = Differential(x)^2

  # δρₛ(x) = δρ₀*exp(complex(0,1)*k*x);
  λ=k^2*c₀^2/100
  ω=k*(vₕ+c₀)
  γ=complex(λ,ω)
#+end_src

#+RESULTS:

Work out analytically only the real part,
\begin{equation}
  \begin{aligned}
    \Re(\delta{\rho})=&\Re(\delta{\rho_0}.e^{ikx}.e^{-\gamma{t}}) \\
    \Leftrightarrow &\delta{\rho_0}.cos(kx).\Re(e^{-\gamma{t}})\\
    \Leftrightarrow &\delta{\rho_0}.cos(kx).\Re(e^{-(\lambda+i\omega)t})\\
    \Leftrightarrow &\delta{\rho_0}.cos(kx).\Re(e^{-(\lambda{t})}.e^{-(i\omega)t})\\
    \Leftrightarrow &\delta{\rho_0}.cos(kx).e^{-(\lambda{t})}.\cos{\omega{t}}\\
  \end{aligned}
\end{equation}

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  # Complete complex term
  δρ(t,x)=δρ₀*exp(complex(0,k*x))*exp(-γ*t)
  δv(t,x)=δv₀*exp(complex(0,k*x))*exp(-γ*t)
  # Only real part
  δρᵣ(t,x)=δρ₀*cos(k*x)*cos(ω*t)exp(-λ*t)
  δvᵣ(t,x)=δv₀*cos(k*x)*cos(ω*t)exp(-λ*t)
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output
  δρᵣ(0,1)
#+end_src

#+RESULTS:
: 0.019999605217122744

 # $V(\rho) = v_h * (1 + e^{\frac{(\rho - \rho_h)}{\tau}})^{-1} + \delta{v}$

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  #2D PDE
  eqs  = [Dt(v(t,x)) + v(t,x)*Dx(v(t,x)) - (μ/ρ(t,x))*Dxx(v(t,x)) + (c₀^2/ρ(t,x))*Dx(ρ(t,x)) - (5.0461*((1+exp((ρ(t,x)-0.25)/0.06))^-1 - 3.72*10^-6) - v(t,x))/τ ~ 0,
	  Dt(ρ(t,x)) + Dx(ρ(t,x)*v(t,x)) ~ 0]
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output  :tangle neuralPDE.jl
  # Initial and boundary conditions
  bcs = [ρ(t,0) ~ ρ(t,L),
	 v(t,0) ~ v(t,L),
	 Dt(v(t,0)) ~ Dt(v(t,L)),
	 # max(ρ(t,x)) ~ ρₕ,
	 ρ(0,x) ~ ρₕ + δρᵣ(0,x),
	 v(0,x) ~ vₕ + δvᵣ(0,x)]

  # Space and time domains
  domains = [t ∈ Interval(0.0,2000.0),
	     x ∈ Interval(0.0,L)]

  # Discretization
  dx = 0.1
#+end_src

#+RESULTS:

*** Workinng

    #+begin_src julia :session main :result output :tangle neuralPDE.jl
  import Flux: flatten, params
    #+end_src

    #+RESULTS:
    : nothing

#+begin_src julia :session main :result output :tangle neuralPDE.jl 
# Neural network
  input_ = length(domains)
  n = 15
  # Neural network
  dim = 2 # number of dimensions
  chain = FastChain(FastDense(dim,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))
#+end_src

#+RESULTS:
: FastChain{Tuple{FastDense{typeof(σ), DiffEqFlux.var"#initial_params#90"{Vector{Float32}}}, FastDense{typeof(σ), DiffEqFlux.var"#initial_params#90"{Vector{Float32}}}, FastDense{typeof(identity), DiffEqFlux.var"#initial_params#90"{Vector{Float32}}}}}((FastDense{typeof(σ), DiffEqFlux.var"#initial_params#90"{Vector{Float32}}}(16, 2, NNlib.σ, DiffEqFlux.var"#initial_params#90"{Vector{Float32}}(Float32[-0.26870885, -0.49727193, 0.058522884, 0.51397544, 0.4966507, 0.4856737, -0.23375209, -0.25797632, 0.42476657, -0.57276636, 0.06396506, 0.5494009, -0.55324566, -0.402425, -0.117099866, 0.25584978, 0.32989472, -0.29875228, -0.3603598, -0.53825146, -0.16470717, 0.35419068, 0.51356375, -0.07399885, -0.48424408, -0.084303275, -0.43352875, -0.29844505, -0.12068953, 0.24632004, 0.5189404, -0.32948768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), true), FastDense{typeof(σ), DiffEqFlux.var"#initial_params#90"{Vector{Float32}}}(16, 16, NNlib.σ, DiffEqFlux.var"#initial_params#90"{Vector{Float32}}(Float32[-0.105565056, -0.036575977, -0.21446522, 0.29384318, 0.36129957, -0.07521342, 0.09155531, -0.2438779, -0.4073363, -0.07359567, 0.36196607, 0.35186216, -0.27813402, 0.41949922, 0.39378908, -0.18122528, 0.21737397, -0.16664124, -0.2919411, 0.025609491, 0.15995562, -0.08400653, 0.1723913, -0.39315405, -0.10942565, -0.114256166, 0.3207009, -0.20443408, 0.14976497, -0.057814155, 0.2184463, 0.3216412, 0.1347344, 0.3488226, 0.17618261, 0.38406423, -0.0669112, 0.13823532, 0.224373, -0.06913692, 0.17010435, -0.3006089, -0.14687368, 0.06862351, -0.3052731, -0.2541105, 0.31651863, 0.034790367, 0.07984241, 0.21723345, -0.15222101, 0.13093565, -0.3753691, -0.34878895, -0.11371314, 0.20942761, -0.3313988, 0.044250917, 0.088629335, 0.41977364, 0.35474095, 0.1562407, -0.35545236, -0.20806497, -0.40320057, -0.17414841, 0.25222668, -0.0082894135, -0.11714344, 0.3319545, 0.2355985, -0.22811963, -0.18882589, -0.30999553, 0.0013278506, -0.33919966, 0.29364422, -0.30884823, -0.08593771, -0.29888234, -0.39970866, 0.22841199, -0.31821916, 0.41373944, -0.101661205, 0.09117622, 0.001107024, -0.2975486, 0.15107228, -0.1513598, 0.4236714, -0.052586786, 0.27720663, -0.41809425, 0.26771656, 0.17339714, -0.29732683, -0.23963451, -0.10217884, 0.3885736, 0.35956982, -0.39501956, -0.23149872, -0.09045138, 0.35796705, -0.37853777, 0.0617816, 0.19520706, -0.35925257, -0.34328872, 0.05368669, -0.2364365, -0.10961695, -0.41669753, -0.1977363, -0.2569008, 0.13316281, -0.2205471, 0.3180899, -0.13716082, 0.3102305, 0.14616701, -0.21895537, 0.18744735, -0.26626563, 0.40924436, 0.2812605, -0.40499467, -0.32906032, 0.18653245, -0.26321557, 0.234677, 0.24360794, 0.12191996, 0.37397185, 0.16466752, 0.24301411, 0.1492082, -0.21687563, 0.28140604, 0.041470505, -0.26596656, -0.10605771, 0.29946378, -0.14479683, 0.18458074, 0.269956, 0.20513125, 0.29532805, 0.33951744, 0.23907381, 0.25061792, 0.015276889, 0.29005826, -0.123972334, 0.12326392, -0.14014596, -0.015911598, 0.14726546, -0.41928303, -0.3352544, 0.22585642, -0.095478676, -0.1794181, -0.18159096, -0.14853653, 0.22496198, 0.03917944, 0.051038627, 0.3393789, -0.16704994, 0.10294342, 0.43079028, -0.38575384, 0.17007668, 0.3239012, -0.06084946, 0.2826092, 0.28774747, -0.18507597, -0.3110531, -0.055605475, -0.052901976, -0.09553711, -0.0011687605, -0.20037197, 0.19441749, 0.124741666, 0.036112335, 0.43261346, 0.07581922, 0.023203317, 0.28031638, 0.21059947, -0.2097877, 0.2004372, -0.2389616, -0.1189118, -0.035758536, -0.02028694, 0.10969624, -0.1305101, 0.12905031, 0.28759962, -0.23655987, 0.17123647, -0.05369185, 0.2718686, 0.32737693, 0.3155713, -0.12378279, -0.3294781, 0.42768654, 0.37124863, -0.0046116537, -0.42657733, -0.20843446, -0.26783207, 0.13908528, -0.06599692, 0.36864817, -0.1395453, -0.027120797, -0.32958695, -0.079687245, 0.29278383, 0.24993645, 0.043886486, -0.30953094, -0.015355247, -0.24416089, -0.18488044, 0.204961, 0.25758135, -0.09899394, 0.29533002, 0.4313388, -0.4296241, -0.22950166, 0.12988769, 0.008070651, -0.30234784, 0.2840347, 0.27001414, -0.0031255386, -0.029005619, 0.17629586, -0.39919358, -0.28086737, 0.07278216, 0.12761778, 0.09773939, -0.0890236, -0.3059239, 0.03955502, 0.22463988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), true), FastDense{typeof(identity), DiffEqFlux.var"#initial_params#90"{Vector{Float32}}}(1, 16, identity, DiffEqFlux.var"#initial_params#90"{Vector{Float32}}(Float32[0.14042334, 0.36524338, -0.008456579, -0.14150833, -0.58230126, 0.3392878, -0.3316162, -0.31290746, 0.06484416, 0.17310463, -0.21630777, -0.078160465, 0.22974122, -0.08728531, 0.33382255, -0.20417799, 0.0]), true)))

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  discretization = PhysicsInformedNN(chain, QuadratureTraining()) 
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  pde_system = PDESystem(eqs,bcs,domains,[t,x],[v,ρ])
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  prob = discretize(pde_system,discretization)
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  cb = function (p,l)
      println("Current loss is: $l")
      return false
  end
  
  res = GalacticOptim.solve(prob, ADAM(0.1); cb = cb, maxiters=100)
  prob = remake(prob,u0=res.minimizer)
  res = GalacticOptim.solve(prob, ADAM(0.1); cb = cb, maxiters=100)
  phi = discretization.phi
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output
  ts,xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]
  v_predict_contourf = reshape([first(phi([t,x],res.minimizer)) for t in ts for x in xs] ,length(xs),length(ts))
  plot(ts, xs, v_predict_contourf, linetype=:contourf,title = "predict")
  
  v_predict = [[first(phi([t,x],res.minimizer)) for x in xs] for t in ts ]
  ρ_predict = [[second(phi([t,x],res.minimizer)) for x in xs] for t in ts ]
  
  p1= plot(xs, v_predict[3],title = "t = 0.1");
  p2= plot(xs, v_predict[11],title = "t = 0.5");
  p3= plot(xs, v_predict[end],title = "t = 1");
  plot(p1,p2,p3)
#+end_src

#+RESULTS:

*** Tests
#+begin_src julia :session main :result output :tangle neuralPDE.jl
# Neural network
  input_ = length(domains)
  n = 5
  chain =[FastChain(FastDense(input_,n,Flux.σ),FastDense(n,n,Flux.σ),FastDense(n,1)) for _ in 1:2]
  initθ = map(c -> Float64.(c), DiffEqFlux.initial_params.(chain))
  flat_initθ = reduce(vcat,initθ)
  
  eltypeθ = eltype(initθ[1])
  parameterless_type_θ = DiffEqBase.parameterless_type(initθ[1])
  phi = NeuralPDE.get_phi.(chain,parameterless_type_θ)
  
  map(phi_ -> phi_(rand(2,10), flat_initθ),phi)
  
  derivative = NeuralPDE.get_numeric_derivative()
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl 
  # :tangle neuralPDE.jl
    indvars = [t,x]
    depvars = [v,ρ]
    dim = length(domains)
    quadrature_strategy = NeuralPDE.QuadratureTraining()


    _pde_loss_functions = [NeuralPDE.build_loss_function(eq,indvars,depvars,phi,derivative,
                                                         chain,initθ,quadrature_strategy) for eq in  eqs]

    map(loss_f -> loss_f(rand(2,10), flat_initθ),_pde_loss_functions)

    bc_indvars = NeuralPDE.get_argument(bcs,indvars,depvars)
    _bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars, phi, derivative,
                                                        chain,initθ,quadrature_strategy,
                                                        bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]
    map(loss_f -> loss_f(rand(1,10), flat_initθ),_bc_loss_functions)

#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  # dx = 0.1
  # train_sets = NeuralPDE.generate_training_sets(domains,dx,eqs,bcs,eltypeθ,indvars,depvars)
  # pde_train_set,bcs_train_set = train_sets
  pde_bounds, bcs_bounds = NeuralPDE.get_bounds(domains,eqs,bcs,eltypeθ,indvars,depvars,quadrature_strategy)
  
  plbs,pubs = pde_bounds
  pde_loss_functions = [NeuralPDE.get_loss_function(_loss,
                                                   lb,ub,
                                                   eltypeθ, parameterless_type_θ,
                                                   quadrature_strategy)
                                                   for (_loss,lb,ub) in zip(_pde_loss_functions, plbs,pubs)]
  
  map(l->l(flat_initθ) ,pde_loss_functions)
  
  blbs,bubs = bcs_bounds
  bc_loss_functions = [NeuralPDE.get_loss_function(_loss,lb,ub,
                                                   eltypeθ, parameterless_type_θ,
                                                   quadrature_strategy)
                                                   for (_loss,lb,ub) in zip(_bc_loss_functions, blbs,bubs)]
  
  map(l->l(flat_initθ) ,bc_loss_functions)
  
  loss_functions =  [pde_loss_functions;bc_loss_functions]
  
  function loss_function(θ,p)
      sum(map(l->l(θ) ,loss_functions))
  end
  
  f_ = OptimizationFunction(loss_function, GalacticOptim.AutoZygote())
  prob = GalacticOptim.OptimizationProblem(f_, flat_initθ)
  
  cb_ = function (p,l)
      println("loss: ", l )
      println("pde losses: ", map(l -> l(p), loss_functions[1:2]))
      println("bcs losses: ", map(l -> l(p), loss_functions[3:end]))
      return false
  end
  
  res = GalacticOptim.solve(prob,Optim.BFGS(); cb = cb_, maxiters=5)
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl
ts,xs = [infimum(d.domain):0.1:supremum(d.domain) for d in domains]

acum =  [0;accumulate(+, length.(initθ))]
sep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]
minimizers_ = [res.minimizer[s] for s in sep]

u_predict  = [[phi[i]([t,x],minimizers_[i])[1] for t in ts  for x in xs] for i in 1:2]
#+end_src

#+RESULTS:

#+begin_src julia :session main :result output :tangle neuralPDE.jl
  for i in 1:2
      p1 = plot(ts, xs, u_predict[i],linetype=:contourf,title = "predict$i");
      plot(p1)
      savefig("./sol$i")
  end
#+end_src

#+RESULTS:


